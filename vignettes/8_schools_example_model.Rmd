---
title: "8 Schools Example Model"
output:
  html_document:
    css: greta.css
    toc: yes
    toc_float:
      collapsed: false
    toc_depth: 3
    theme: lumen
    highlight: default
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{Example models}
  %\usepackage[utf8]{inputenc}
---

## Data

**8 Schools** is a study of coaching effects from eight schools; it comes from [section 5.5 of Gelman et al. (2003) as covered in 2.1. Schools data of 'R2WinBUGS: A Package for Running WinBUGS from R'](https://www.jstatsoft.org/article/view/v012i03/v12i03.pdf):

> The Scholastic Aptitude Test (SAT) measures the aptitude of high-schoolers in order to help
colleges to make admissions decisions. It is divided into two parts, verbal (SAT-V) and
mathematical (SAT-M). Our data comes from the SAT-V (Scholastic Aptitude Test-Verbal)
on eight different high schools, from an experiment conducted in the late 1970s. SAT-V is a
standard multiple choice test administered by the Educational Testing Service. This Service
was interested in the effects of coaching programs for each of the selected schools.
The study included coached and uncoached pupils, about sixty in each of the eight different
schools; see Rubin (1981). All of them had already taken the PSAT (Preliminary SAT)
which results were used as covariates. For each school, the estimated treatment effect and
the standard error of the effect estimate are given. These are calculated by an analysis of
covariance adjustment appropriate for a completely randomized experiment (Rubin 1981).
This example was analyzed using a hierarchical normal model in Rubin (1981) and Gelman,
Carlin, Stern, and Rubin (2003, Section 5.5).

The corresponding [TensorFlow Probability](https://medium.com/tensorflow/introducing-tensorflow-probability-dca4c304e245) Jupyter notebook can be found [here](https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Eight_Schools.ipynb).

```{r libraries, warning=FALSE, message=FALSE}
library(greta)
library(ggplot2)
```

```{r 8_schools_data, highlight = FALSE}
# data
N <- letters[1:8]
treatment_effects <- c(28.39, 7.94, -2.75 , 6.82, -0.64, 0.63, 18.01, 12.16)
treatment_stddevs <- c(14.9, 10.2, 16.3, 11.0, 9.4, 11.4, 10.4, 17.6)
```

For each the eight schools `N` we have the estimated treatment effect (`treatment_effects`) plus standard error (`treatment_stddevs`).
Below, we are replicating the barplot from the [TensorFlow Probability example](https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Eight_Schools.ipynb):
 
```{r}
schools <- data.frame(N = N,
                      treatment_effects = treatment_effects,
                      treatment_stddevs = treatment_stddevs)

ggplot(schools, aes(x = N, y = treatment_effects)) +
  geom_bar(stat = "identity") +
  geom_errorbar(aes(ymin = treatment_effects - treatment_stddevs, ymax = treatment_effects + treatment_stddevs), width = 0.5) +
  labs(x = "School", treatment_effects = "Treatment effect",
       title = "8 schools treatment effects",
       subtitle = "Error bars represent standard error")
```

```{r}
ggplot(schools, aes(x = treatment_effects)) +
  geom_density(fill = "black", alpha = 0.5)
```

## Modeling with `greta`

To model the data, we use the same hierarchical normal model as in the [TensorFlow Probability example](https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Eight_Schools.ipynb).

### Variables and priors

First, we create greta arrays that represent the variables and prior distributions in our model and create a greta array for school effect from them. We define the following (random) variables and priors:

- `avg_effect`: normal density function (`dnorm`) with a mean of `0` and standard deviation of `10`; represents the prior average treatment effect.

```{r}
avg_effect <- normal(mean = 0, sd = 10)
avg_effect
```

- `avg_stddev`: normal density function (`dnorm`) with a mean of `5` and standard deviation of `1`; controls the amount of variance between schools.

```{r}
avg_stddev <- normal(5, 1)
avg_stddev
```

- `school_effects_standard`: normal density function (`dnorm`) with a mean of `0`, standard deviation of `1` and dimension of `8`

```{r}
school_effects_standard <- normal(0, 1, dim = length(N))
school_effects_standard
```

- `school_effects`: here we multiply the exponential of `avg_stddev` with `school_effects_standard` and add `avg_effect`

```{r}
school_effects <- avg_effect + exp(avg_stddev) * school_effects_standard
school_effects
```

An alternative would be to directly use the `lognormal()` density function for `avg_stddev` and use that to calculate `school_effect`: 

```{r}
avg_stddev <- lognormal(5, 1)
school_effects <- avg_effect + avg_stddev * school_effects_standard
```

### Likelihood

Next, we want to link the variables and priors with the observed dependent data - in this case the school estimate `treatment_effects`. 
We define the likelihood over our observed estimates `treatment_effects` given a random sample from the normal probability distribution with mean `school_effects` and standard deviation `treatment_stddevs`. From this, we would now like to calculate the parameter of that probability distribution by using the `distribution()` function:

```{r}
distribution(treatment_effects) <- normal(school_effects, treatment_stddevs)
```

### Bayesian inference model

Now we have all the prerequisites for building a Hamiltonian Monte Carlo (HMC) to calculate the posterior distribution over the model's parameters.

We first define the model by combining the calculated `avg_effect`, `avg_stddev` and `school_effects_standard` variables so that we can sample from them during modeling. The model `m` we define below contains all our prior distributions and thus represent the combined density of the model.

It is recommended that you check your model at this step by plotting the model graph. More information about these plots can be found [here](https://greta-dev.github.io/greta/get_started.html#plotting).

```{r 8_schools_greta}
# defining the hierarchical model
m <- model(avg_effect, avg_stddev, school_effects_standard)
m

# plotting
plot(m)
```

The actual sampling from the model happens with the `mcmc()` function. By default 1000 MCMC samples are drawn after warm-up.


When defining the model, greta combines all of the distributions together to define the joint density of the model, a measure of how ‘good’ (or how probable if we’re being Bayesian) are a particular candidate set of values for the variables in the model.

Now we have a greta model that will give us the joint density for a candidate set of values, so we can use that to carry out inference on the model. We do that using an Markov chain Monte Carlo (MCMC) algorithm to sample values of the parameters we’re interested in, using the mcmc() function:

Here we’re using 1000 steps of the static Hamiltonian Monte Carlo (HMC) algorithm, which uses the gradients of the joint density to efficiently explore the set of parameters. By default, greta also spends 100 iterations ‘warming up’ (tuning the sampler parameters) and ‘burning in’ (moving to the area of highest probability) the sampler.

draws is an mcmc.list object, from the coda R package. So we can use functions from coda, or one of the many other MCMC software packages that use this format, to plot and summarise the MCMC samples.


```{r}
# sampling
draws <- greta::mcmc(m)
plot(draws)
```

## Session information

```{r}
sessionInfo()
```

